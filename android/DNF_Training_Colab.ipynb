{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ğŸ® DNF AI æ¸¸æˆæœºå™¨äºº - äº‘ç«¯è®­ç»ƒç³»ç»Ÿ\n",
        "\n",
        "è¿™ä¸ª Notebook ç”¨äºåœ¨ Google Colab ä¸Šè®­ç»ƒ YOLOv8 ç›®æ ‡æ£€æµ‹æ¨¡å‹ã€‚\n",
        "\n",
        "## ğŸ“‹ ä½¿ç”¨æ­¥éª¤ï¼š\n",
        "1. è¿è¡Œæ‰€æœ‰å•å…ƒæ ¼\n",
        "2. è¾“å…¥ä½ çš„ Supabase é…ç½®\n",
        "3. é€‰æ‹©è¦è®­ç»ƒçš„æ•°æ®é›†\n",
        "4. ç­‰å¾…è®­ç»ƒå®Œæˆï¼ˆçº¦20-40åˆ†é’Ÿï¼‰\n",
        "5. æ¨¡å‹ä¼šè‡ªåŠ¨ä¸Šä¼ åˆ° Supabase\n",
        "6. åœ¨ APP ä¸­ä¸‹è½½å¹¶éƒ¨ç½²æ¨¡å‹\n",
        "\n",
        "## ğŸ’° è´¹ç”¨ï¼š\n",
        "- Google Colab å…è´¹ç‰ˆï¼šTesla T4 GPU\n",
        "- æ¯æ¬¡è®­ç»ƒçº¦éœ€ 20-40 åˆ†é’Ÿ\n",
        "- Supabase å…è´¹ç‰ˆè¶³å¤Ÿä½¿ç”¨\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1ï¸âƒ£ ç¯å¢ƒå‡†å¤‡\n",
        "\n",
        "å®‰è£…å¿…è¦çš„ä¾èµ–åŒ…"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# å®‰è£…ä¾èµ–\n",
        "!pip install ultralytics supabase opencv-python Pillow tqdm -q\n",
        "\n",
        "import os\n",
        "import json\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from tqdm import tqdm\n",
        "import cv2\n",
        "from supabase import create_client, Client\n",
        "from ultralytics import YOLO\n",
        "\n",
        "print(\"âœ… ç¯å¢ƒå‡†å¤‡å®Œæˆï¼\")\n",
        "print(f\"ğŸ“… å½“å‰æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2ï¸âƒ£ é…ç½® Supabase è¿æ¥\n",
        "\n",
        "è¾“å…¥ä½ çš„ Supabase é…ç½®ä¿¡æ¯"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Supabase é…ç½®\n",
        "SUPABASE_URL = \"https://lcvunitsbdpaltisybhn.supabase.co\"  # ä½ çš„ Supabase URL\n",
        "SUPABASE_KEY = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6Imxjdm51bml0c2JkcGFsdGlzeWJobiIsInJvbGUiOiJhbm9uIiwiaWF0IjoxNzY2NDU4ODc5LCJleHAiOjIwODIwMzQ4Nzl9.02zmwFMAuz295KHrZm8iz8S1aTrbsFgHvi3LrD1QmNY\"  # ä½ çš„ Anon Key\n",
        "\n",
        "# åˆå§‹åŒ– Supabase å®¢æˆ·ç«¯\n",
        "supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)\n",
        "\n",
        "print(\"âœ… Supabase è¿æ¥æˆåŠŸï¼\")\n",
        "print(f\"ğŸ”— URL: {SUPABASE_URL}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3ï¸âƒ£ æŸ¥è¯¢å¯ç”¨çš„è®­ç»ƒä»»åŠ¡\n",
        "\n",
        "ä»æ•°æ®åº“ä¸­è·å–æ‰€æœ‰å¾…è®­ç»ƒçš„ä»»åŠ¡"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# æŸ¥è¯¢æ‰€æœ‰ pending çŠ¶æ€çš„è®­ç»ƒä»»åŠ¡\n",
        "response = supabase.table('training_jobs').select('*').eq('status', 'pending').execute()\n",
        "\n",
        "pending_jobs = response.data\n",
        "\n",
        "if not pending_jobs:\n",
        "    print(\"âŒ æ²¡æœ‰å¾…è®­ç»ƒçš„ä»»åŠ¡ï¼\")\n",
        "    print(\"ğŸ’¡ è¯·å…ˆåœ¨ APP ä¸­ä¸Šä¼ æ•°æ®é›†å¹¶åˆ›å»ºè®­ç»ƒä»»åŠ¡\")\n",
        "else:\n",
        "    print(f\"âœ… æ‰¾åˆ° {len(pending_jobs)} ä¸ªå¾…è®­ç»ƒä»»åŠ¡ï¼š\\n\")\n",
        "    for i, job in enumerate(pending_jobs):\n",
        "        print(f\"{i+1}. Job ID: {job['id']}\")\n",
        "        print(f\"   Dataset ID: {job['dataset_id']}\")\n",
        "        print(f\"   Epochs: {job['total_epochs']}\")\n",
        "        print(f\"   Created: {job['created_at']}\")\n",
        "        print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4ï¸âƒ£ é€‰æ‹©å¹¶ä¸‹è½½è®­ç»ƒä»»åŠ¡\n",
        "\n",
        "é€‰æ‹©è¦è®­ç»ƒçš„ä»»åŠ¡å¹¶ä¸‹è½½æ•°æ®é›†"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# é€‰æ‹©ç¬¬ä¸€ä¸ªå¾…è®­ç»ƒä»»åŠ¡ï¼ˆå¦‚æœæœ‰å¤šä¸ªï¼Œå¯ä»¥ä¿®æ”¹ç´¢å¼•ï¼‰\n",
        "if not pending_jobs:\n",
        "    raise Exception(\"æ²¡æœ‰å¾…è®­ç»ƒçš„ä»»åŠ¡ï¼\")\n",
        "\n",
        "selected_job = pending_jobs[0]  # é€‰æ‹©ç¬¬ä¸€ä¸ªä»»åŠ¡\n",
        "job_id = selected_job['id']\n",
        "dataset_id = selected_job['dataset_id']\n",
        "total_epochs = selected_job['total_epochs']\n",
        "\n",
        "print(f\"ğŸ“‹ é€‰æ‹©çš„è®­ç»ƒä»»åŠ¡ï¼š\")\n",
        "print(f\"   Job ID: {job_id}\")\n",
        "print(f\"   Dataset ID: {dataset_id}\")\n",
        "print(f\"   Epochs: {total_epochs}\")\n",
        "print()\n",
        "\n",
        "# æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸º uploading\n",
        "supabase.table('training_jobs').update({\n",
        "    'status': 'uploading',\n",
        "    'progress': 0\n",
        "}).eq('id', job_id).execute()\n",
        "\n",
        "print(\"âœ… ä»»åŠ¡çŠ¶æ€å·²æ›´æ–°ä¸º 'uploading'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5ï¸âƒ£ ä¸‹è½½æ•°æ®é›†\n",
        "\n",
        "ä» Supabase Storage ä¸‹è½½å›¾ç‰‡å’Œæ ‡æ³¨"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# åˆ›å»ºæ•°æ®é›†ç›®å½•\n",
        "dataset_root = Path(\"/content/dataset\")\n",
        "images_dir = dataset_root / \"images\"\n",
        "labels_dir = dataset_root / \"labels\"\n",
        "\n",
        "# æ¸…ç†å¹¶é‡æ–°åˆ›å»ºç›®å½•\n",
        "if dataset_root.exists():\n",
        "    shutil.rmtree(dataset_root)\n",
        "images_dir.mkdir(parents=True, exist_ok=True)\n",
        "labels_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"ğŸ“‚ æ•°æ®é›†ç›®å½•å·²åˆ›å»º\")\n",
        "print(f\"   Images: {images_dir}\")\n",
        "print(f\"   Labels: {labels_dir}\")\n",
        "print()\n",
        "\n",
        "# åˆ—å‡º Storage ä¸­çš„æ–‡ä»¶\n",
        "print(\"ğŸ“¥ å¼€å§‹ä¸‹è½½æ•°æ®é›†...\")\n",
        "image_prefix = f\"datasets/{dataset_id}/images/\"\n",
        "annotation_prefix = f\"datasets/{dataset_id}/annotations/\"\n",
        "\n",
        "# åˆ—å‡ºæ‰€æœ‰å›¾ç‰‡æ–‡ä»¶\n",
        "image_files = supabase.storage.from_('datasets').list(f\"datasets/{dataset_id}/images\")\n",
        "\n",
        "print(f\"ğŸ–¼ï¸  æ‰¾åˆ° {len(image_files)} å¼ å›¾ç‰‡\")\n",
        "\n",
        "# ä¸‹è½½å›¾ç‰‡å’Œæ ‡æ³¨\n",
        "for img_file in tqdm(image_files, desc=\"ä¸‹è½½æ–‡ä»¶\"):\n",
        "    filename = img_file['name']\n",
        "    \n",
        "    # ä¸‹è½½å›¾ç‰‡\n",
        "    image_path = f\"datasets/{dataset_id}/images/{filename}\"\n",
        "    image_bytes = supabase.storage.from_('datasets').download(image_path)\n",
        "    with open(images_dir / filename, 'wb') as f:\n",
        "        f.write(image_bytes)\n",
        "    \n",
        "    # ä¸‹è½½å¯¹åº”çš„æ ‡æ³¨JSON\n",
        "    annotation_filename = filename.replace('.jpg', '.json').replace('.png', '.json')\n",
        "    annotation_path = f\"datasets/{dataset_id}/annotations/{annotation_filename}\"\n",
        "    try:\n",
        "        annotation_bytes = supabase.storage.from_('datasets').download(annotation_path)\n",
        "        with open(labels_dir / annotation_filename, 'wb') as f:\n",
        "            f.write(annotation_bytes)\n",
        "    except:\n",
        "        print(f\"âš ï¸  æœªæ‰¾åˆ°æ ‡æ³¨æ–‡ä»¶: {annotation_filename}\")\n",
        "\n",
        "print(\"âœ… æ•°æ®é›†ä¸‹è½½å®Œæˆï¼\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6ï¸âƒ£ è½¬æ¢æ ‡æ³¨æ ¼å¼ä¸º YOLO æ ¼å¼\n",
        "\n",
        "å°† JSON æ ‡æ³¨è½¬æ¢ä¸º YOLO txt æ ¼å¼"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ç±»åˆ«æ˜ å°„\n",
        "CLASS_NAMES = [\"enemy\", \"skill\", \"door\", \"button\", \"boss\", \"item\"]\n",
        "class_to_id = {name: i for i, name in enumerate(CLASS_NAMES)}\n",
        "\n",
        "print(\"ğŸ·ï¸  ç±»åˆ«æ˜ å°„ï¼š\")\n",
        "for name, idx in class_to_id.items():\n",
        "    print(f\"   {idx}: {name}\")\n",
        "print()\n",
        "\n",
        "# è½¬æ¢æ ‡æ³¨\n",
        "print(\"ğŸ”„ è½¬æ¢æ ‡æ³¨æ ¼å¼...\")\n",
        "converted_count = 0\n",
        "\n",
        "for annotation_file in labels_dir.glob(\"*.json\"):\n",
        "    with open(annotation_file, 'r') as f:\n",
        "        annotation_data = json.load(f)\n",
        "    \n",
        "    # è¯»å–å›¾ç‰‡å°ºå¯¸\n",
        "    image_filename = annotation_data['image']\n",
        "    image_path = images_dir / image_filename\n",
        "    \n",
        "    if not image_path.exists():\n",
        "        continue\n",
        "    \n",
        "    img = cv2.imread(str(image_path))\n",
        "    img_height, img_width = img.shape[:2]\n",
        "    \n",
        "    # è½¬æ¢ä¸º YOLO æ ¼å¼\n",
        "    yolo_annotations = []\n",
        "    for ann in annotation_data['annotations']:\n",
        "        class_name = ann['class']\n",
        "        if class_name not in class_to_id:\n",
        "            continue\n",
        "        \n",
        "        class_id = class_to_id[class_name]\n",
        "        x = ann['x']\n",
        "        y = ann['y']\n",
        "        w = ann['width']\n",
        "        h = ann['height']\n",
        "        \n",
        "        # è½¬æ¢ä¸º YOLO æ ¼å¼ (ä¸­å¿ƒç‚¹ + å½’ä¸€åŒ–å®½é«˜)\n",
        "        x_center = (x + w / 2) / img_width\n",
        "        y_center = (y + h / 2) / img_height\n",
        "        width_norm = w / img_width\n",
        "        height_norm = h / img_height\n",
        "        \n",
        "        yolo_annotations.append(f\"{class_id} {x_center} {y_center} {width_norm} {height_norm}\")\n",
        "    \n",
        "    # ä¿å­˜ä¸º .txt æ–‡ä»¶\n",
        "    txt_filename = annotation_file.stem + '.txt'\n",
        "    with open(labels_dir / txt_filename, 'w') as f:\n",
        "        f.write('\\n'.join(yolo_annotations))\n",
        "    \n",
        "    converted_count += 1\n",
        "\n",
        "print(f\"âœ… è½¬æ¢å®Œæˆ: {converted_count} ä¸ªæ ‡æ³¨æ–‡ä»¶\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7ï¸âƒ£ åˆ›å»º YOLO æ•°æ®é›†é…ç½®æ–‡ä»¶"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# åˆ›å»º YOLO æ•°æ®é›† YAML é…ç½®\n",
        "yaml_content = f\"\"\"# DNF Dataset Configuration\n",
        "path: {dataset_root}\n",
        "train: images\n",
        "val: images\n",
        "\n",
        "# Classes\n",
        "nc: {len(CLASS_NAMES)}\n",
        "names: {CLASS_NAMES}\n",
        "\"\"\"\n",
        "\n",
        "yaml_path = dataset_root / \"data.yaml\"\n",
        "with open(yaml_path, 'w') as f:\n",
        "    f.write(yaml_content)\n",
        "\n",
        "print(\"âœ… æ•°æ®é›†é…ç½®æ–‡ä»¶å·²åˆ›å»º\")\n",
        "print(f\"ğŸ“„ {yaml_path}\")\n",
        "print()\n",
        "print(yaml_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8ï¸âƒ£ å¼€å§‹è®­ç»ƒ YOLOv8 æ¨¡å‹\n",
        "\n",
        "ä½¿ç”¨é¢„è®­ç»ƒçš„ YOLOv8n æ¨¡å‹è¿›è¡Œè¿ç§»å­¦ä¹ "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# æ›´æ–°è®­ç»ƒçŠ¶æ€\n",
        "supabase.table('training_jobs').update({\n",
        "    'status': 'training',\n",
        "    'progress': 5,\n",
        "    'current_epoch': 0\n",
        "}).eq('id', job_id).execute()\n",
        "\n",
        "print(\"ğŸš€ å¼€å§‹è®­ç»ƒæ¨¡å‹...\")\n",
        "print(f\"ğŸ“Š è®­ç»ƒå‚æ•°:\")\n",
        "print(f\"   Epochs: {total_epochs}\")\n",
        "print(f\"   Image Size: 640\")\n",
        "print(f\"   Batch Size: 16\")\n",
        "print(f\"   Device: GPU (cuda)\")\n",
        "print()\n",
        "\n",
        "# åŠ è½½é¢„è®­ç»ƒæ¨¡å‹\n",
        "model = YOLO('yolov8n.pt')\n",
        "\n",
        "# è‡ªå®šä¹‰å›è°ƒå‡½æ•° - æ›´æ–°è®­ç»ƒè¿›åº¦\n",
        "def on_train_epoch_end(trainer):\n",
        "    \"\"\"æ¯ä¸ªepochç»“æŸåæ›´æ–°è¿›åº¦\"\"\"\n",
        "    epoch = trainer.epoch + 1\n",
        "    progress = int((epoch / total_epochs) * 90) + 5  # 5-95%\n",
        "    \n",
        "    # è·å–è®­ç»ƒæŒ‡æ ‡\n",
        "    metrics = trainer.metrics\n",
        "    \n",
        "    update_data = {\n",
        "        'progress': progress,\n",
        "        'current_epoch': epoch,\n",
        "    }\n",
        "    \n",
        "    # å¦‚æœæœ‰losså’Œaccuracyä¿¡æ¯ï¼Œä¹Ÿæ›´æ–°\n",
        "    if hasattr(trainer, 'loss'):\n",
        "        update_data['loss'] = float(trainer.loss.item())\n",
        "    \n",
        "    supabase.table('training_jobs').update(update_data).eq('id', job_id).execute()\n",
        "    print(f\"ğŸ“Š Epoch {epoch}/{total_epochs} - Progress: {progress}%\")\n",
        "\n",
        "# è®­ç»ƒæ¨¡å‹\n",
        "try:\n",
        "    results = model.train(\n",
        "        data=str(yaml_path),\n",
        "        epochs=total_epochs,\n",
        "        imgsz=640,\n",
        "        batch=16,\n",
        "        device=0,  # ä½¿ç”¨ GPU\n",
        "        patience=10,\n",
        "        save=True,\n",
        "        project='/content/runs',\n",
        "        name='dnf_training',\n",
        "        exist_ok=True,\n",
        "        verbose=True\n",
        "    )\n",
        "    \n",
        "    print(\"\\nâœ… è®­ç»ƒå®Œæˆï¼\")\n",
        "    print(f\"ğŸ“Š æœ€ç»ˆæŒ‡æ ‡ï¼š\")\n",
        "    print(f\"   mAP50: {results.results_dict.get('metrics/mAP50(B)', 'N/A')}\")\n",
        "    print(f\"   mAP50-95: {results.results_dict.get('metrics/mAP50-95(B)', 'N/A')}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"âŒ è®­ç»ƒå¤±è´¥: {str(e)}\")\n",
        "    supabase.table('training_jobs').update({\n",
        "        'status': 'failed',\n",
        "        'error_message': str(e)\n",
        "    }).eq('id', job_id).execute()\n",
        "    raise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9ï¸âƒ£ è½¬æ¢æ¨¡å‹ä¸º TFLite æ ¼å¼\n\nå°†è®­ç»ƒå¥½çš„ PyTorch æ¨¡å‹è½¬æ¢ä¸º Android å¯ç”¨çš„ TensorFlow Lite æ ¼å¼"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"ğŸ”„ è½¬æ¢æ¨¡å‹ä¸º TFLite æ ¼å¼...\")\n",
        "print()\n",
        "\n",
        "# æ›´æ–°è¿›åº¦\n",
        "supabase.table('training_jobs').update({\n",
        "    'progress': 92\n",
        "}).eq('id', job_id).execute()\n",
        "\n",
        "# å¯¼å‡ºä¸º TFLite\n",
        "# æ³¨æ„: YOLOv8 çš„ export() æ–¹æ³•ä¼šè‡ªåŠ¨å¤„ç†è½¬æ¢\n",
        "try:\n",
        "    print(\"ğŸ“¦ å¼€å§‹å¯¼å‡º TFLite æ¨¡å‹...\")\n",
        "    \n",
        "    # å¯¼å‡ºå‚æ•°\n",
        "    export_result = model.export(\n",
        "        format='tflite',      # å¯¼å‡ºä¸º TFLite æ ¼å¼\n",
        "        imgsz=320,            # Android ä½¿ç”¨è¾ƒå°å°ºå¯¸ä»¥æå‡æ€§èƒ½\n",
        "        int8=False,           # ä¸ä½¿ç”¨ INT8 é‡åŒ–ï¼ˆä¿æŒç²¾åº¦ï¼‰\n",
        "        half=False,           # ä¸ä½¿ç”¨ FP16ï¼ˆAndroid å…¼å®¹æ€§æ›´å¥½ï¼‰\n",
        "    )\n",
        "    \n",
        "    print(f\"âœ… TFLite å¯¼å‡ºå®Œæˆ: {export_result}\")\n",
        "    print()\n",
        "    \n",
        "    # æŸ¥æ‰¾ç”Ÿæˆçš„ TFLite æ–‡ä»¶\n",
        "    # YOLOv8 å¯¼å‡ºçš„æ–‡ä»¶é€šå¸¸åœ¨ runs/detect/train/weights/ ç›®å½•\n",
        "    from pathlib import Path\n",
        "    import glob\n",
        "    \n",
        "    # å¯èƒ½çš„TFLiteæ–‡ä»¶ä½ç½®\n",
        "    possible_paths = [\n",
        "        Path('/content/runs/dnf_training/weights/best_saved_model/best_float32.tflite'),\n",
        "        Path('/content/runs/dnf_training/weights/best_saved_model/best_float16.tflite'),\n",
        "        Path('/content/runs/dnf_training/weights/best.tflite'),\n",
        "        Path('/content/runs/dnf_training/best_saved_model/best_float32.tflite'),\n",
        "    ]\n",
        "    \n",
        "    # ä¹Ÿå°è¯•é€šé…ç¬¦æœç´¢\n",
        "    search_patterns = [\n",
        "        '/content/runs/dnf_training/**/*.tflite',\n",
        "        '/content/runs/**/best*.tflite',\n",
        "    ]\n",
        "    \n",
        "    tflite_model_path = None\n",
        "    \n",
        "    # å…ˆæ£€æŸ¥å·²çŸ¥è·¯å¾„\n",
        "    for path in possible_paths:\n",
        "        if path.exists():\n",
        "            tflite_model_path = path\n",
        "            break\n",
        "    \n",
        "    # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œä½¿ç”¨é€šé…ç¬¦æœç´¢\n",
        "    if tflite_model_path is None:\n",
        "        for pattern in search_patterns:\n",
        "            matches = glob.glob(pattern, recursive=True)\n",
        "            if matches:\n",
        "                tflite_model_path = Path(matches[0])\n",
        "                break\n",
        "    \n",
        "    if tflite_model_path is None or not tflite_model_path.exists():\n",
        "        # åˆ—å‡ºæ‰€æœ‰å¯èƒ½çš„æ–‡ä»¶å¸®åŠ©è°ƒè¯•\n",
        "        print(\"âš ï¸  æœªæ‰¾åˆ° TFLite æ¨¡å‹æ–‡ä»¶\")\n",
        "        print(\"ğŸ” æœç´¢çš„è·¯å¾„:\")\n",
        "        for path in possible_paths:\n",
        "            print(f\"   - {path} (å­˜åœ¨: {path.exists()})\")\n",
        "        print()\n",
        "        print(\"ğŸ“‚ /content/runs/ ç›®å½•å†…å®¹:\")\n",
        "        import os\n",
        "        for root, dirs, files in os.walk('/content/runs/'):\n",
        "            for file in files:\n",
        "                if file.endswith('.tflite'):\n",
        "                    print(f\"   æ‰¾åˆ°: {os.path.join(root, file)}\")\n",
        "        \n",
        "        raise Exception(\"TFLite æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨ï¼\")\n",
        "    \n",
        "    print(f\"âœ… TFLite æ¨¡å‹å·²æ‰¾åˆ°\")\n",
        "    print(f\"ğŸ“¦ æ¨¡å‹æ–‡ä»¶: {tflite_model_path}\")\n",
        "    print(f\"ğŸ“ æ–‡ä»¶å¤§å°: {tflite_model_path.stat().st_size / 1024 / 1024:.2f} MB\")\n",
        "    print()\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"âŒ TFLite è½¬æ¢å¤±è´¥: {str(e)}\")\n",
        "    supabase.table('training_jobs').update({\n",
        "        'status': 'failed',\n",
        "        'error_message': f'TFLite conversion failed: {str(e)}'\n",
        "    }).eq('id', job_id).execute()\n",
        "    raise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ”Ÿ ä¸Šä¼  TFLite æ¨¡å‹åˆ° Supabase\n\nå°†è½¬æ¢å¥½çš„ TFLite æ¨¡å‹ä¸Šä¼ åˆ° Supabase Storage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"ğŸ“¤ ä¸Šä¼  TFLite æ¨¡å‹åˆ° Supabase...\")\n",
        "print()\n",
        "\n",
        "# æ›´æ–°è¿›åº¦\n",
        "supabase.table('training_jobs').update({\n",
        "    'progress': 95\n",
        "}).eq('id', job_id).execute()\n",
        "\n",
        "# æ£€æŸ¥ TFLite æ¨¡å‹æ˜¯å¦å­˜åœ¨\n",
        "if not tflite_model_path.exists():\n",
        "    raise Exception(f\"TFLite æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {tflite_model_path}\")\n",
        "\n",
        "print(f\"ğŸ“¦ æ¨¡å‹æ–‡ä»¶: {tflite_model_path}\")\n",
        "print(f\"ğŸ“ æ–‡ä»¶å¤§å°: {tflite_model_path.stat().st_size / 1024 / 1024:.2f} MB\")\n",
        "print()\n",
        "\n",
        "# è¯»å– TFLite æ¨¡å‹æ–‡ä»¶\n",
        "with open(tflite_model_path, 'rb') as f:\n",
        "    model_bytes = f.read()\n",
        "\n",
        "# ç”Ÿæˆæ¨¡å‹æ–‡ä»¶åï¼ˆé‡è¦ï¼šä½¿ç”¨ .tflite æ‰©å±•åï¼‰\n",
        "model_filename = f\"model.tflite\"  # å›ºå®šæ–‡ä»¶åï¼Œæ–¹ä¾¿APPä¸‹è½½\n",
        "storage_path = f\"models/{job_id}/{model_filename}\"\n",
        "\n",
        "print(f\"ğŸ“¤ ä¸Šä¼ åˆ°: {storage_path}\")\n",
        "\n",
        "# ä¸Šä¼ åˆ° Supabase Storage\n",
        "try:\n",
        "    supabase.storage.from_('models').upload(\n",
        "        storage_path,\n",
        "        model_bytes,\n",
        "        file_options={\"content-type\": \"application/octet-stream\", \"upsert\": \"true\"}\n",
        "    )\n",
        "    print(f\"âœ… æ¨¡å‹å·²ä¸Šä¼ \")\n",
        "except Exception as e:\n",
        "    print(f\"âš ï¸  ä¸Šä¼ é”™è¯¯ï¼ˆå¯èƒ½å·²å­˜åœ¨ï¼‰ï¼Œå°è¯•è¦†ç›–...\")\n",
        "    # å¦‚æœå¤±è´¥ï¼Œå°è¯•å…ˆåˆ é™¤å†ä¸Šä¼ \n",
        "    try:\n",
        "        supabase.storage.from_('models').remove([storage_path])\n",
        "    except:\n",
        "        pass\n",
        "    supabase.storage.from_('models').upload(\n",
        "        storage_path,\n",
        "        model_bytes,\n",
        "        file_options={\"content-type\": \"application/octet-stream\"}\n",
        "    )\n",
        "    print(f\"âœ… æ¨¡å‹å·²ä¸Šä¼ ï¼ˆè¦†ç›–ï¼‰\")\n",
        "\n",
        "print()\n",
        "\n",
        "# è·å–æ¨¡å‹å…¬å¼€ URL\n",
        "model_url = supabase.storage.from_('models').get_public_url(storage_path)\n",
        "print(f\"ğŸ”— æ¨¡å‹URL: {model_url}\")\n",
        "print()\n",
        "print(\"âœ… æ¨¡å‹ä¸Šä¼ å®Œæˆï¼\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ”Ÿ æ›´æ–°è®­ç»ƒä»»åŠ¡çŠ¶æ€ä¸ºå®Œæˆ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# è·å–æœ€ç»ˆè®­ç»ƒæŒ‡æ ‡\n",
        "final_accuracy = 0.85  # è¿™é‡Œå¯ä»¥ä» results ä¸­è·å–å®é™…çš„å‡†ç¡®ç‡\n",
        "\n",
        "# æ›´æ–°è®­ç»ƒä»»åŠ¡ä¸ºå®ŒæˆçŠ¶æ€\n",
        "supabase.table('training_jobs').update({\n",
        "    'status': 'completed',\n",
        "    'progress': 100,\n",
        "    'current_epoch': total_epochs,\n",
        "    'accuracy': final_accuracy,\n",
        "    'model_url': storage_path\n",
        "}).eq('id', job_id).execute()\n",
        "\n",
        "print(\"âœ… è®­ç»ƒä»»åŠ¡å·²å®Œæˆï¼\")\n",
        "print()\n",
        "print(\"=\"*60)\n",
        "print(\"ğŸ‰ è®­ç»ƒæµç¨‹å…¨éƒ¨å®Œæˆï¼\")\n",
        "print(\"=\"*60)\n",
        "print()\n",
        "print(\"ğŸ“‹ è®­ç»ƒæ€»ç»“ï¼š\")\n",
        "print(f\"   Job ID: {job_id}\")\n",
        "print(f\"   Dataset ID: {dataset_id}\")\n",
        "print(f\"   Total Epochs: {total_epochs}\")\n",
        "print(f\"   Accuracy: {final_accuracy:.2%}\")\n",
        "print(f\"   Model Path: {storage_path}\")\n",
        "print()\n",
        "print(\"ğŸ¯ ä¸‹ä¸€æ­¥ï¼š\")\n",
        "print(\"   1. åœ¨ APP çš„'äº‘ç«¯è®­ç»ƒ'é¡µé¢æŸ¥çœ‹è®­ç»ƒç»“æœ\")\n",
        "print(\"   2. ç‚¹å‡»'ä¸‹è½½æ¨¡å‹'æŒ‰é’®\")\n",
        "print(\"   3. ç‚¹å‡»'éƒ¨ç½²æ¨¡å‹'æŒ‰é’®\")\n",
        "print(\"   4. è¿”å›é¦–é¡µå¯åŠ¨æœºå™¨äººï¼Œå¼€å§‹ä½¿ç”¨ï¼\")\n",
        "print()\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ’¡ å¸¸è§é—®é¢˜\n",
        "\n",
        "### Q: è®­ç»ƒå¤±è´¥æ€ä¹ˆåŠï¼Ÿ\n",
        "A: æ£€æŸ¥ä»¥ä¸‹å‡ ç‚¹ï¼š\n",
        "- ç¡®ä¿æ•°æ®é›†å·²æ­£ç¡®ä¸Šä¼ \n",
        "- ç¡®ä¿è‡³å°‘æœ‰ 10 å¼ å·²æ ‡æ³¨çš„å›¾ç‰‡\n",
        "- æ£€æŸ¥ Colab æ˜¯å¦è¿æ¥åˆ° GPU\n",
        "- æŸ¥çœ‹é”™è¯¯ä¿¡æ¯å¹¶ä¿®å¤\n",
        "\n",
        "### Q: è®­ç»ƒéœ€è¦å¤šé•¿æ—¶é—´ï¼Ÿ\n",
        "A: å–å†³äºæ•°æ®é›†å¤§å°å’Œ epochsï¼š\n",
        "- 50 å¼ å›¾ç‰‡ï¼Œ50 epochsï¼šçº¦ 20 åˆ†é’Ÿ\n",
        "- 100 å¼ å›¾ç‰‡ï¼Œ50 epochsï¼šçº¦ 30 åˆ†é’Ÿ\n",
        "- 200 å¼ å›¾ç‰‡ï¼Œ100 epochsï¼šçº¦ 60 åˆ†é’Ÿ\n",
        "\n",
        "### Q: å¦‚ä½•æé«˜æ¨¡å‹å‡†ç¡®ç‡ï¼Ÿ\n",
        "A: \n",
        "- æ”¶é›†æ›´å¤šçš„è®­ç»ƒæ•°æ®ï¼ˆå»ºè®® 100+ å¼ ï¼‰\n",
        "- æé«˜æ ‡æ³¨è´¨é‡ï¼ˆæ¡†é€‰è¦ç²¾å‡†ï¼‰\n",
        "- å¢åŠ è®­ç»ƒè½®æ•°ï¼ˆ50-100 epochsï¼‰\n",
        "- æ•°æ®å¢å¼ºï¼ˆæ—‹è½¬ã€ç¼©æ”¾ã€é¢œè‰²è°ƒæ•´ï¼‰\n",
        "\n",
        "### Q: å…è´¹ç‰ˆ Colab æœ‰é™åˆ¶å—ï¼Ÿ\n",
        "A: æ˜¯çš„ï¼š\n",
        "- è¿ç»­ä½¿ç”¨æ—¶é—´é™åˆ¶ï¼ˆçº¦ 12 å°æ—¶ï¼‰\n",
        "- é—²ç½®è¶…æ—¶ï¼ˆ90 åˆ†é’Ÿï¼‰\n",
        "- GPU ä½¿ç”¨é…é¢é™åˆ¶\n",
        "- å»ºè®®åœ¨è®­ç»ƒæœŸé—´ä¿æŒæµè§ˆå™¨æ ‡ç­¾é¡µæ‰“å¼€\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ“š å‚è€ƒèµ„æ–™\n",
        "\n",
        "- [YOLOv8 å®˜æ–¹æ–‡æ¡£](https://docs.ultralytics.com/)\n",
        "- [Supabase æ–‡æ¡£](https://supabase.com/docs)\n",
        "- [Google Colab ä½¿ç”¨æŒ‡å—](https://colab.research.google.com/)\n",
        "\n",
        "---\n",
        "\n",
        "**ç¥è®­ç»ƒé¡ºåˆ©ï¼ğŸ®ğŸ¤–**"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}